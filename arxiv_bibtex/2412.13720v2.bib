@article{2412.13720v2,
  title={ Federated Learning and RAG Integration: A Scalable Approach for Medical Large Language Models },
  author={ Jincheol Jung and Hongju Jeong and Eui-Nam Huh },
  year={ 2024 },
  journal={ arXiv preprint arXiv:2412.13720v2 },
  url={ http://arxiv.org/abs/2412.13720v2 },
  eprint={ 2412.13720v2 },
  archivePrefix={arXiv},
  primaryClass={ cs.CL },
  pdf={ http://arxiv.org/pdf/2412.13720v2 },
  abstract={ This study analyzes the performance of domain-specific Large Language Models (LLMs) for the medical field by integrating Retrieval-Augmented Generation (RAG) systems within a federated learning framework. Leveraging the inherent advantages of federated learning, such as preserving data privacy and enabling distributed computation, this research explores the integration of RAG systems with models trained under varying client configurations to optimize performance. Experimental results demonstrate that the federated learning-based models integrated with RAG systems consistently outperform their non-integrated counterparts across all evaluation metrics. This study highlights the potential of combining federated learning and RAG systems for developing domain-specific LLMs in the medical field, providing a scalable and privacy-preserving solution for enhancing text generation capabilities. }
}
