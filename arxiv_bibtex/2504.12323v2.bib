@article{2504.12323v2,
  title={ The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation },
  author={ Zheng Zhang and Ning Li and Qi Liu and Rui Li and Weibo Gao and Qingyang Mao and Zhenya Huang and Baosheng Yu and Dacheng Tao },
  year={ 2025 },
  journal={ arXiv preprint arXiv:2504.12323v2 },
  url={ http://arxiv.org/abs/2504.12323v2 },
  eprint={ 2504.12323v2 },
  archivePrefix={arXiv},
  primaryClass={ cs.CL },
  pdf={ http://arxiv.org/pdf/2504.12323v2 },
  abstract={ Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant document from external knowledge sources. By referencing this external knowledge, RAG effectively reduces the generation of factually incorrect content and addresses hallucination issues within LLMs. Recently, there has been growing attention to improving the performance and efficiency of RAG systems from various perspectives. While these advancements have yielded significant results, the application of RAG in domains with considerable societal implications raises a critical question about fairness: What impact does the introduction of the RAG paradigm have on the fairness of LLMs? To address this question, we conduct extensive experiments by varying the LLMs, retrievers, and retrieval sources. Our experimental analysis reveals that the scale of the LLMs plays a significant role in influencing fairness outcomes within the RAG framework. When the model scale is smaller than 8B, the integration of retrieval mechanisms often exacerbates unfairness in small-scale LLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness issues introduced by RAG for small-scale LLMs, we propose two approaches, FairFT and FairFilter. Specifically, in FairFT, we align the retriever with the LLM in terms of fairness, enabling it to retrieve documents that facilitate fairer model outputs. In FairFilter, we propose a fairness filtering mechanism to filter out biased content after retrieval. Finally, we validate our proposed approaches on real-world datasets, demonstrating their effectiveness in improving fairness while maintaining performance. }
}
